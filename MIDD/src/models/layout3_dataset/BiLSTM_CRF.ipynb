{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from functions.split_dataset import split_dataset, prepare_to_eval\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Input, Dropout\n",
    "from tensorflow.keras.layers import TimeDistributed, Bidirectional\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from seqeval.metrics import f1_score, classification_report, accuracy_score\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow_addons.layers import CRF\n",
    "from tensorflow_addons.losses import SigmoidFocalCrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "# for later versions: \n",
    "# tf.compat.v1.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../../data/input/IOB/Layout3'\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(dataset_path):\n",
    "    file_path = os.path.join(dataset_path, filename)\n",
    "    df_file = pd.read_csv(file_path)[['Text','Tag']]\n",
    "    df_file['id'] = filename.replace('.csv', '')\n",
    "    df = pd.concat([df, df_file], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = list(set(df[\"id\"].values))\n",
    "num_documents = len(all_documents)\n",
    "\n",
    "all_tags = list(set(df[\"Tag\"].values))\n",
    "num_tags = len(all_tags)\n",
    "\n",
    "all_words = list(set(df[\"Text\"].values))\n",
    "num_words = len(all_words)\n",
    "\n",
    "print(f'Quantidade de documentos: {num_documents}')\n",
    "print(f'Number of unique words: {num_words}')\n",
    "print(f'Quantidade de tags: {num_tags}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Tag\"]!=\"O\"][\"Tag\"].value_counts().plot(kind=\"bar\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = df.groupby(\"id\")[\"Text\"].agg([\"count\"])\n",
    "word_counts = word_counts.rename(columns={\"count\": \"Word count\"})\n",
    "word_counts.hist(bins=50, figsize=(8,6))\n",
    "word_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE = word_counts.max()[0]\n",
    "print(f'Longest sentence contains {MAX_SENTENCE} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_idx_dict = {tag: idx for idx, tag in enumerate(all_tags)}\n",
    "idx_tag_dict = {idx: tag for idx, tag in enumerate(all_tags)}\n",
    "\n",
    "tag_idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tuples(data):\n",
    "    iterator = zip(data[\"Text\"].values.tolist(),\n",
    "                   data[\"Tag\"].values.tolist())\n",
    "    return [(word, tag) for word, tag in iterator]\n",
    "\n",
    "sentences = df.groupby(\"id\").apply(to_tuples).tolist()\n",
    "\n",
    "print(sentences[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx = pd.DataFrame()\n",
    "\n",
    "df_idx['invoice_id'] = list(set(df['id']))\n",
    "df_idx['text_tag'] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx['text']=df_idx['text_tag'].apply(lambda text_tag:\" \".join([str(s[0]) for s in text_tag]))\n",
    "df_idx['tag']=df_idx['text_tag'].apply(lambda text_tag:\" \".join([str(s[1]) for s in text_tag]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx['tokenized_text']=df_idx['text'].apply(lambda x:x.split())\n",
    "df_idx['tag_list']=df_idx['tag'].apply(lambda x:x.split())\n",
    "df_idx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_list=df_idx['text'].tolist()\n",
    "tags_list=df_idx['tag_list'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=False, filters='!?~[]()^_{\"}\\'%')\n",
    "tokenizer.fit_on_texts(texts_list)\n",
    "encoded_text_list = tokenizer.texts_to_sequences(texts_list)\n",
    "\n",
    "print(\"Vocab size of Tokenizer \",len(tokenizer.word_index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tags_list = [[tag_idx_dict[w] for w in tag] for tag in tags_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=502\n",
    "\n",
    "padded_encoded_text_list = pad_sequences(maxlen=max_len, sequences=encoded_text_list, padding=\"post\", value=0)\n",
    "padded_encoded_tags_list = pad_sequences(maxlen=max_len, sequences=encoded_tags_list, padding=\"post\", value=tag_idx_dict['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tags = [to_categorical(i, num_classes = num_tags) for i in padded_encoded_tags_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-val-test split\n",
    "X, y = split_dataset(padded_encoded_text_list, dummy_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional\n",
    "from tensorflow_addons.layers import CRF\n",
    "from tensorflow_addons.losses import SigmoidFocalCrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=502\n",
    "embedding_dim=100\n",
    "vocab_size=len(tokenizer.word_index)\n",
    "lstm_units=50\n",
    "\n",
    "input_word = Input(shape=(max_len,))\n",
    "\n",
    "model = Embedding(input_dim=vocab_size+1, output_dim=embedding_dim, input_length=max_len)(input_word)\n",
    "\n",
    "model = Bidirectional(LSTM(units=lstm_units, return_sequences=True))(model)\n",
    "model = Dropout(0.1)(model)\n",
    "\n",
    "# out = Dense(lstm_units, activation=\"softmax\")(model)\n",
    "\n",
    "model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  \n",
    "crf = CRF(19)  # CRF layer\n",
    "decoded_sequence, potentials, sequence_length, chain_kernel = crf(model)  # output\n",
    "\n",
    "model = Model(input_word, potentials)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.00001), \n",
    "                loss=SigmoidFocalCrossEntropy(), \n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X['train'],\n",
    "                    np.array(y['train']), \n",
    "                    validation_data=(X['val'], np.array(y['val'])),\n",
    "                    batch_size=512,\n",
    "                    callbacks=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2),\n",
    "                    epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X['test']) ## Predict using model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_index_array = np.argmax(y_pred, axis=-1)\n",
    "test_index_array = np.argmax(y['test'], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_tag, pred_tag = prepare_to_eval(idx_tag_dict, test_index_array, pred_index_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(real_tag, pred_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(real_tag, pred_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(real_tag, pred_tag, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad522d30e3dfc0e14e6af11be4b6def96f4bde859e5802fc4a9db0a62ee0582b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
